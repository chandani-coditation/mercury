# NOC Agent AI - Technical Context

**Source of Truth for Codebase Development and Maintenance**

This document contains all technical details, architecture, configuration, and implementation details needed to understand, develop, and maintain the codebase.

**Note**: This document reflects both the current implementation and the target architecture specification. Items marked as "target" indicate planned changes to align with the specification.

**Configuration-Driven Design**: The system uses configuration files (especially `config/field_mappings.json`) to map source data fields to internal models. This means adding new CSV columns or runbook sections requires only updating the configuration file, not code changes.

---

## Project Deliverables

The NOC Agent AI project consists of three main deliverables:

### 1. Resolution Copilot
- **Purpose**: Recommends remediation steps based on runbooks, relevant logs, and ticket history
- **Implementation**: `ai_service/agents/resolution_copilot.py` and `ai_service/agents/langgraph_wrapper.py` **IMPLEMENTED**
- **LLM Framework**: LangGraph  **IMPLEMENTED** (with backward compatibility)
- **LLM Model**: GPT-4o-mini  **IMPLEMENTED** in JSON mode
- **Input**:
  - Alert/ticket details
  - Evidence chunks from retriever (runbooks, past incidents, logs)
  - Policy band (AUTO/PROPOSE/REVIEW) from Policy Gate
- **Output** (strict JSON):
  - `steps[]`: Ordered natural language actions (safe, actionable) **IMPLEMENTED**
    - **Source**: Extracted from runbooks (DOCX files) - primary source
  - `commands_by_step`: Optional terminal commands copied directly from runbooks (structured by step - dict mapping step index to commands array) **IMPLEMENTED**
    - **Source**: Extracted from runbooks (DOCX files) - commands section
  - `confidence`: System's confidence in these steps (0.0-1.0)  **IMPLEMENTED**
    - **Source**: Calculated by LLM based on evidence quality and runbook match
  - `reasoning`: Short explanation citing which evidence chunks justify the steps **IMPLEMENTED**
    - **Source**: Generated by LLM citing runbooks, historical incidents, and logs
  - `provenance[]`: Array of `{doc_id, chunk_id}` references (audit trail) **IMPLEMENTED**
    - **Source**: References to retrieved chunks from Context Lake (runbooks, incidents, logs)
  - `rollback_plan`: Optional rollback steps **IMPLEMENTED**
    - **Source**: Extracted from runbooks (DOCX files) - rollback procedures section
  - `estimated_time_minutes`: Time estimate **IMPLEMENTED**
    - **Source**: Inferred from historical incidents or runbook estimates
  - `risk_level`: Risk assessment (low, medium, high)  **IMPLEMENTED**
    - **Source**: Derived from severity, commands, and historical patterns
- **Feedback Loop**: Analyst can edit steps (reorder, delete, add new, mark unsafe) or give thumbs/notes → stored as `system_output`, `user_edited`, `diff` in Postgres
- **API Endpoint**: `POST /api/v1/resolution?incident_id={id}`
- **Data Sources**: Runbooks (primary), ServiceNow tickets (context), logs (via InfluxDB - secondary)

### 2. Alert Triager
- **Purpose**: Identifies priority, probable cause, and routing for new alerts
- **Implementation**: `ai_service/agents/triager.py` and `ai_service/agents/langgraph_wrapper.py` **IMPLEMENTED**
- **LLM Framework**: LangGraph  **IMPLEMENTED** (with backward compatibility)
- **LLM Model**: GPT-4o-mini  **IMPLEMENTED** in JSON mode
- **Input**:
  - Alert/ticket metadata (id, title, description, tags)
  - Evidence chunks from retriever (logs, runbooks, past incidents)
- **Output** (strict JSON):
  - `severity`: Severity level (Sev1, Sev2, Sev3, Sev4 or critical, high, medium, low)
    - **Source**: Derived from ServiceNow `impact` and `urgency` fields (1-5 scale)
  - `probable_cause`: Root cause explanation (e.g., "Redis session bloat")
    - **Source**: Inferred from `description` field and historical patterns
  - `routing`: Team queue assignment (e.g., "SE DBA SQL", "NOC", "SE Windows") - **REQUIRED** **IMPLEMENTED**
    - **Source**: **Maps directly to `assignment_group` field from ServiceNow tickets**
  - `confidence`: Confidence score (0.0-1.0)
    - **Source**: Calculated by LLM based on evidence quality
  - `reasoning`: Short natural text citing evidence chunks **IMPLEMENTED** (in prompts, ready for LLM output)
    - **Source**: Generated by LLM citing retrieved evidence
  - `category`: Incident category (database, network, application, infrastructure, security, other)
    - **Source**: Maps directly to `category` field from ServiceNow tickets
  - `affected_services`: List of impacted services
    - **Source**: Derived from `cmdb_ci` field (e.g., "Database-SQL", "Server")
  - `recommended_actions`: List of recommended actions
    - **Source**: Inferred from historical incidents and runbooks
  - `summary`: Brief summary of the alert/incident
    - **Source**: Derived from `short_description` field from ServiceNow tickets
- **Feedback Loop**: Analyst can override JSON → stored as `system_output`, `user_edited`, `diff` in Postgres
- **API Endpoint**: `POST /api/v1/triage`
- **Data Sources**: ServiceNow tickets (historical incidents), runbooks (context), logs (via InfluxDB)

### 3. Context Lake
- **Purpose**: A lightweight repository of logs, alerts, incidents, and runbooks used for retrieval and demonstration
- **Implementation**: PostgreSQL database with pgvector (vector search) and tsvector (full-text search)
- **Key Components**:
  - **Documents Table**: Stores source documents (runbooks, historical incidents, logs, alerts)
  - **Chunks Table**: Chunked documents with embeddings and full-text vectors
- **Hybrid Search**: 
  - **RRF (Reciprocal Rank Fusion)**: Combines pgvector and tsvector results to find the winner
  - **MMR (Maximal Marginal Relevance)**: Enforces diversity in the resulting set
- **Ingestion Pipeline**: JSON schema-driven parser/normalizer converting raw data into searchable chunks
  - Sentence-safe split using tiktoken
  - Headers and metadata auto-appended to chunks
  - Comprehensive tags attached to all documents
- **Data Sources**:
  - **Runbooks**: DOCX files from `runbooks/` folder (primary for resolution)
    - Parsed using JSON schema-driven extraction (steps, commands, sections)
  - **Historical Incidents**: ServiceNow tickets from `tickets_data/` folder (CSV format)
    - Normalized JSON with key fields + description
  - **Logs**: Retrieved via InfluxDB (read-only cURL queries)
    - Secondary priority but integrated for context
  - **Alerts**: Normalized JSON (key fields + description)
- **Storage**: PostgreSQL with pgvector extension
- **Retrieval**: Hybrid search via `retrieval/hybrid_search.py` using RRF and MMR
- **Embeddings**: text-embedding-3-large (OpenAI) or OSS alternative (e.g., bge-m3)

---

## Table of Contents

1. [Project Deliverables](#project-deliverables)
2. [Architecture Overview](#architecture-overview)
3. [Code Structure](#code-structure)
4. [Configuration System](#configuration-system)
5. [Data Sources](#data-sources)
6. [Data Flow](#data-flow)
7. [Guardrails and Limitations](#guardrails-and-limitations)
8. [Database Schema](#database-schema)
9. [Error Handling](#error-handling)
10. [Key Implementation Details](#key-implementation-details)
11. [Deployment and Infrastructure](#deployment-and-infrastructure)

---

## Architecture Overview

### Technology Stack

- **Backend**: Python (FastAPI)
- **LLM Agent Framework**: LangGraph **IMPLEMENTED** (with backward compatibility for custom state-based)
- **Database**: PostgreSQL with pgvector (vector similarity) and tsvector (full-text search)
- **Hybrid Search**: 
  - **RRF (Reciprocal Rank Fusion)**: Combines pgvector and tsvector results to find the winner
  - **MMR (Maximal Marginal Relevance)**: Enforces diversity in the resulting set
- **Parser/Normalizer**: Python (FastAPI service) with schema validation (Pydantic)
  - **Runbook & Incident Parsing**: JSON schema–driven extraction (steps, commands, sections)
  - **Logs**: Retrieved via InfluxDB (read-only cURL queries)
  - **Alerts**: Normalized JSON (key fields + description)
- **Chunking**: 
- **Sentence-safe Split**: Tokenizer (tiktoken) targeting 180-320 tokens with 30 token overlap (within 20-40 range)
  - **Headers & Metadata**: Compact headers auto-appended (doc type, service/component, title, last_reviewed_at)
  - **Tags**: Mandatory fields attached (type, service, component, env, runbook_id, incident_id, canonical_incident_key, risk, last_reviewed_at, ticket_id)
  - **Validation & QA**: Schema + rule-based checks (min ≥120 tokens, max ≤360, required tags present)
- **Embeddings**: text-embedding-3-large (OpenAI) or OSS alternative (e.g., bge-m3) - **target**
  - **Current**: text-embedding-3-small
- **LLM Model**: GPT-4o-mini (current) in JSON mode

The system architecture supports the three main deliverables:
- **Alert Triager** (Agent Layer) - Analyzes and triages incoming alerts
- **Resolution Copilot** (Agent Layer) - Generates remediation recommendations
- **Context Lake** (Repository Layer + Database) - Stores and retrieves knowledge base

### System Components

```
┌─────────────────┐
│   API Layer     │  FastAPI endpoints (/api/v1/*)
├─────────────────┤
│  Service Layer  │  Business logic orchestration
├─────────────────┤
│  Agent Layer    │  LangGraph nodes (Alert Triager, Resolution Copilot)
├─────────────────┤
│ Repository Layer│  Data access abstraction (Context Lake)
├─────────────────┤
│   Core Layer    │  Logging, config, exceptions
└─────────────────┘
```

### Architecture Layers

#### 1. API Layer (`ai_service/api/v1/`)
- **Purpose**: HTTP endpoints, request/response handling
- **Responsibilities**:
  - Route requests to services or agents
  - Validate input (Pydantic models)
  - Handle HTTP exceptions
  - Return JSON responses
- **No business logic** - delegates to services or agents
- **Files**: `health.py`, `triage.py`, `resolution.py`, `incidents.py`, `feedback.py`, `calibration.py`, `simulate.py`

#### 2. Service Layer (`ai_service/services/`)
- **Purpose**: Business logic orchestration
- **Responsibilities**:
  - Coordinate between agents, repositories, and policies
  - Enforce business rules
  - Handle transactions
  - Provide reusable business logic
- **No direct database access** - uses repositories
- **Files**: `incident_service.py`, `feedback_service.py`

#### 3. Repository Layer (`ai_service/repositories/`)
- **Purpose**: Data access abstraction for Context Lake
- **Responsibilities**:
  - Database operations (CRUD) for Context Lake
  - Query building for hybrid search
  - Data mapping
  - Transaction management
- **Context Lake Components**:
  - Documents storage (runbooks, incidents, logs)
  - Chunks with embeddings (vector search)
  - Full-text search indexes (tsvector)
- **Benefits**:
  - Easy to test (can mock repositories)
  - Easy to swap databases
  - Centralized data access
  - Type-safe operations
- **Files**: `incident_repository.py`, `feedback_repository.py`
- **Note**: Uses `dict_row` factory - `fetchone()` returns dictionaries, not tuples

#### 4. Agent Layer (`ai_service/agents/`)
- **Purpose**: AI agent implementations using LangGraph framework **IMPLEMENTED**
- **Framework**: LangGraph nodes for agent orchestration **IMPLEMENTED**
- **Components**:
  - **Alert Triager** (`triager.py` or `langgraph_wrapper.py`): LangGraph node for triage analysis **IMPLEMENTED**
    - **LLM Model**: GPT-4o-mini (current) in JSON mode
    - **Input**: Alert/ticket metadata (id, title, description, tags) + Evidence chunks from retriever
    - **Output**: Severity (Sev1, Sev2), Probable cause, Routing (team queue), Confidence score, Reasoning (citing evidence chunks)
  - **Resolution Copilot** (`resolution_copilot.py` or `langgraph_wrapper.py`): LangGraph node for resolution generation **IMPLEMENTED**
    - **LLM Model**: GPT-4o-mini (current) in JSON mode
    - **Input**: Alert/ticket details + Evidence chunks + Policy band (AUTO/PROPOSE/REVIEW)
    - **Output**: steps[], commands_by_step[], confidence, reasoning, provenance[]
- **Responsibilities**:
  - Context retrieval from Context Lake (hybrid search)
  - LLM calls in JSON mode (via `llm_client.py`)
  - Output validation (via `guardrails.py`)
  - Evidence tracking with provenance
  - Policy evaluation (via `policy.py`)
- **Uses**: Repositories, LLM client, guardrails, policy, retrieval
- **Files**: `triager.py`, `resolution_copilot.py`, `langgraph_wrapper.py` **NEW**
- **LangGraph Integration**: **COMPLETE**
  - Full node logic implemented (retrieval, LLM, validation, policy, storage)
  - API endpoints support LangGraph via feature flag (`USE_LANGGRAPH` env var or `use_langgraph` query param)
  - Backward compatible - existing agents still work by default
  - Enable: Set `USE_LANGGRAPH=true` or use `?use_langgraph=true` in API calls

#### 5. Core Layer (`ai_service/core/`)
- **Purpose**: Shared infrastructure
- **Components**:
  - **Logging** (`logger.py`): Structured logging with configurable levels, daily log rotation
  - **Configuration** (`config_loader.py`): Dynamic config loading and caching
  - **Exceptions** (`exceptions.py`): Custom exception hierarchy
- **Benefits**: Centralized, reusable, consistent across the application

---

## Code Structure

### Folder Structure

```
noc_agent_ai/
├── ai_service/              # AI service package
│   ├── api/                 # API routes (separated by version)
│   │   └── v1/              # API v1 endpoints
│   │       ├── health.py    # Health check
│   │       ├── triage.py    # Triage endpoints
│   │       ├── resolution.py # Resolution endpoints
│   │       ├── incidents.py # Incident management
│   │       ├── feedback.py  # Feedback endpoints
│   │       ├── calibration.py # Calibration endpoints
│   │       └── simulate.py   # Simulation endpoints
│   ├── agents/              # Agent implementations
│   │   ├── triager.py       # Triager agent
│   │   └── resolution_copilot.py # Resolution copilot agent
│   ├── core/                # Core utilities
│   │   ├── logger.py        # Logging configuration
│   │   ├── config_loader.py # Configuration loading
│   │   └── exceptions.py    # Custom exceptions
│   ├── repositories/        # Data access layer (Repository pattern)
│   │   ├── incident_repository.py
│   │   └── feedback_repository.py
│   ├── services/            # Business logic layer
│   │   ├── incident_service.py
│   │   └── feedback_service.py
│   ├── models.py            # Pydantic models
│   ├── llm_client.py        # LLM client
│   ├── policy.py            # Policy gates
│   ├── guardrails.py        # Validation guardrails
│   ├── prompts.py           # LLM prompt templates
│   └── main.py              # FastAPI application (thin layer)
├── ingestion/               # Ingestion service
├── retrieval/               # Hybrid search
├── db/                      # Database schema and migrations
├── config/                  # Configuration files (split by concern)
│   ├── policy.json
│   ├── guardrails.json
│   ├── llm.json
│   ├── retrieval.json
│   ├── workflow.json
│   └── schemas.json
├── tests/                   # Unit tests
├── scripts/                 # Utility scripts
├── tickets_data/            # ServiceNow tickets (CSV format)
│   ├── Database Alerts Filtered - Sheet1.csv
│   └── High Disk Filtered - Sheet1.csv
├── runbooks/                # Operational runbooks (DOCX format)
│   ├── Runbook - Database Alerts.docx
│   ├── Runbook - High CPU Alerts.docx
│   ├── Runbook - High Memory Alerts.docx
│   ├── Runbook – High Volume or Disk Utilization.docx
│   └── Runbook – Network Alerts.docx
├── Dockerfile               # Backend Docker image
├── docker-compose.yml       # Full stack deployment
└── README.md
```

### Design Patterns

#### Repository Pattern
- **Why**: Separates data access from business logic
- **Implementation**: `IncidentRepository`, `FeedbackRepository`
- **Benefits**: 
  - Testable (easy to mock)
  - Maintainable (centralized queries)
  - Swappable (can change database without affecting business logic)
  - Type-safe operations

#### Service Layer Pattern
- **Why**: Separates business logic from API layer
- **Implementation**: `IncidentService`, `FeedbackService`
- **Benefits**: 
  - Reusable (can be used by multiple API endpoints)
  - Testable (can test business logic independently)
  - Clear separation of concerns

#### API Versioning
- **Why**: Allows breaking changes without breaking clients
- **Implementation**: `/api/v1/` prefix
- **Future**: Can add `/api/v2/` when needed

#### Dependency Injection
- **Why**: Makes code testable and flexible
- **Implementation**: Services accept repository instances (optional, defaults provided)
- **Benefits**: Easy to mock for testing

---

## Configuration System

### Configuration Files

All configuration is in `config/` directory, split by concern:

| Config File | Purpose | Used In |
|------------|---------|---------|
| `config/policy.json` | Policy gate bands (AUTO/PROPOSE/REVIEW), conditions, actions | `ai_service/policy.py`, `ai_service/agents/resolution_copilot.py`, `ai_service/api/v1/feedback.py` |
| `config/guardrails.json` | Validation rules, allowed values, limits, dangerous commands | `ai_service/guardrails.py` |
| `config/llm.json` | LLM settings (model, temperature, system prompts) | `ai_service/llm_client.py` |
| `config/retrieval.json` | Hybrid search settings (limits, weights, filters, preferred types) | `ai_service/agents/triager.py`, `ai_service/agents/resolution_copilot.py` |
| `config/workflow.json` | Workflow behavior (feedback timing, policy evaluation) | `ai_service/agents/triager.py`, `ai_service/agents/resolution_copilot.py`, `ai_service/api/v1/feedback.py` |
| `config/schemas.json` | Data schema definitions (historical data inputs, alert metadata) | Data ingestion scripts (ServiceNow tickets, runbooks) |
| `config/field_mappings.json` | **Field mappings from source data (CSV columns, DOCX sections) to internal models** | **CSV/DOCX ingestion scripts, normalizers** |
| `config/embeddings.json` | **Embedding model configuration (model selection, dimensions, batch size)** | **`ingestion/embeddings.py`, `ingestion/db_ops.py`** |
| `config/embeddings.json` | **Embedding model configuration (model selection, dimensions, batch size)** | **`ingestion/embeddings.py`, `ingestion/db_ops.py`** |

### Configuration Access

All configuration is accessed via helper functions in `ai_service/core/config_loader.py`:

```python
from ai_service.core import (
    get_policy_config,
    get_guardrail_config,
    get_llm_config,
    get_retrieval_config,
    get_workflow_config,
    get_field_mappings_config  # For CSV/DOCX field mappings
)
```

These functions:
1. Load and cache configuration
2. Return the relevant section
3. Provide defaults if config is missing
4. Log configuration loading for observability

### Configuration Loading Mechanism

- **`core/config_loader.py`** loads and merges all config files
- **Cached** for performance (loaded once, reused)
- **Can be reloaded** at runtime (via `reload_config()`)
- **Type-safe access** via helper functions

### Key Configuration Details

#### Policy Configuration (`config/policy.json`)
- Defines policy bands: AUTO, PROPOSE, REVIEW
- Each band has:
  - `can_auto_apply`: Boolean - whether resolution can proceed automatically
  - `requires_approval`: Boolean - whether user approval is needed
  - `notification_required`: Boolean - whether to notify stakeholders
  - `rollback_required`: Boolean - whether rollback plan is mandatory
- Conditions evaluated in `evaluation_order`
- **Important**: Code uses `can_auto_apply` and `requires_approval` flags, NOT hardcoded band names

#### Guardrails Configuration (`config/guardrails.json`)
- **Triage limits**:
  - `max_summary_length`: 500 characters
  - `max_likely_cause_length`: 300 characters
  - `max_affected_services`: 50 items (increased from 10)
  - `max_recommended_actions`: 50 items (increased from 10)
- **Resolution limits**:
  - `min_resolution_steps`: 1
  - `max_resolution_steps`: 20
  - `min_estimated_time_minutes`: 1
  - `max_estimated_time_minutes`: 1440 (24 hours)
  - `max_commands`: 10
  - `max_rollback_steps`: 10
- **Safety checks**:
  - `dangerous_commands`: List of blocked commands
  - `destructive_patterns`: Regex patterns for dangerous operations
  - `require_rollback_for_risk_levels`: Risk levels requiring rollback

#### Field Mappings Configuration (`config/field_mappings.json`)
- **Purpose**: Maps source data fields (ServiceNow CSV columns, Runbook DOCX sections) to internal models
- **Key Feature**: **Configuration-driven** - add new columns/sections by updating JSON, no code changes needed
- **ServiceNow CSV Mappings**:
  - Maps CSV columns (e.g., `assignment_group`, `cmdb_ci`, `short_description`) to internal fields
  - Maps to TriageOutput fields (e.g., `assignment_group` → `routing`)
  - Severity derivation from `impact` + `urgency` (1-5 scale → critical/high/medium/low)
- **Runbook DOCX Mappings**:
  - Maps DOCX sections (e.g., `steps`, `commands`, `rollback_procedures`) to internal fields
  - Maps to ResolutionOutput fields (e.g., `steps` → `steps[]`, `commands` → `commands_by_step`)
- **Benefits**:
  - Easy to add new CSV columns: just add to `field_mappings.json`
  - Easy to add new runbook sections: just add to `field_mappings.json`
  - No code changes required for new fields
  - Single source of truth for field mappings
- **Location**: `config/field_mappings.json`

#### Retrieval Configuration (`config/retrieval.json`)
- **Triage section**:
  - `limit`: Number of chunks to retrieve (default: 5)
  - `vector_weight`: Weight for vector similarity (default: 0.7)
  - `fulltext_weight`: Weight for full-text search (default: 0.3)
  - `prefer_types`: Preferred document types (typically `["incident", "alert"]` for historical context)
  - `max_per_type`: Maximum chunks per type
- **Resolution section**: Same structure, **optimized for runbook retrieval**
  - `prefer_types`: Should prioritize `["runbook"]` for resolution recommendations
  - Runbooks from `runbooks/` folder are the primary source for resolution steps

#### Workflow Configuration (`config/workflow.json`)
- `feedback_before_policy`: If true, policy is deferred until triage feedback is received
- `feedback_timeout_secs`: Optional timeout for feedback (0 = no timeout)
- `resolution_requires_approval`: If true, require approval even for AUTO band

---

## Data Sources

### Real Data Sources

The system uses **real production data** instead of generated fake data:

1. **ServiceNow Tickets** (`tickets_data/` folder):
   - **Format**: CSV files exported from ServiceNow
   - **Content**: Historical incident tickets with full details
   - **Files**:
     - `Database Alerts Filtered - Sheet1.csv`: Database-related incidents
     - `High Disk Filtered - Sheet1.csv`: Disk/volume-related incidents
   - **Actual CSV Columns**: `number`, `problem_id`, `opened_at`, `u_next_action_due_date`, `short_description`, `cmdb_ci`, `category`, `state`, `assignment_group`, `opened_by`, `impact`, `urgency`, `sys_updated_on`, `description`, `u_reopen_count`
   - **Field Mapping**: All column mappings defined in `config/field_mappings.json` (configuration-driven)
   - **Key Fields for Triage**:
     - `assignment_group`: Maps to `routing` in triage output (e.g., "SE DBA SQL", "NOC")
     - `category`: Maps directly to `category` in triage output
     - `short_description`: Used for `summary` in triage output
     - `cmdb_ci`: Used for `affected_services` in triage output
     - `impact` + `urgency`: Used to derive `severity` in triage output
     - `description`: Used to infer `likely_cause` and `probable_cause`
   - **Key Fields for Resolution** (from historical incidents):
     - `description`: May contain resolution steps → used as fallback for `steps[]` if runbook not found
     - Historical resolution patterns → used to estimate `estimated_time_minutes` and validate `risk_level`
     - Resolution success/failure patterns → used to calculate `confidence`
   - **Usage**: Ingested as historical incidents for triage context retrieval and resolution pattern learning
   - **Status**:  **Ingestion script available** - `scripts/data/ingest_servicenow_tickets.py`
   - **Ingestion**: Uses `config/field_mappings.json` for configuration-driven field mapping

2. **Runbooks** (`runbooks/` folder):
   - **Format**: DOCX (Microsoft Word) files
   - **Content**: Operational runbooks with resolution steps, recommendations, and procedures
   - **Files**:
     - `Runbook - Database Alerts.docx`
     - `Runbook - High CPU Alerts.docx`
     - `Runbook - High Memory Alerts.docx`
     - `Runbook – High Volume or Disk Utilization.docx`
     - `Runbook – Network Alerts.docx`
   - **Key Content for Resolution** (from `IngestRunbook` model):
     - **`steps[]`**: Ordered list of natural language actions → maps to `steps[]` in resolution output
       - **Example**: `["Step 1: Check database connections", "Step 2: Verify disk space", ...]`
     - **Commands**: Terminal commands embedded in steps or separate section → maps to `commands_by_step` in resolution output
       - **Format**: Commands extracted per step, structured as `{"0": ["cmd1"], "1": ["cmd2", "cmd3"]}`
     - **`rollback_procedures`**: Rollback steps → maps to `rollback_plan` in resolution output
       - **Example**: `"1. Stop the service\n2. Restore from backup\n3. Verify status"`
     - **`prerequisites[]`**: Required conditions before execution → used for context in `reasoning`
     - **`title`**: Runbook title → used for provenance tracking
     - **`service`** and **`component`**: Service/component tags → used for filtering and provenance
     - **`content`**: Full runbook text → chunked and stored in Context Lake for retrieval
   - **Usage**: Primary source for resolution recommendations and step-by-step procedures
   - **Status**:  **Ingestion script available** - `scripts/data/ingest_runbooks.py`
   - **Ingestion**: Uses `config/field_mappings.json` for configuration-driven field mapping, JSON schema-driven extraction

3. **Logs** (via InfluxDB):
   - **Source**: InfluxDB time-series database
   - **Access**: Read-only queries via InfluxDB API (Flux queries)
   - **Usage**: Retrieved dynamically during triage/resolution for additional context
   - **Status**:  **Integrated** - `retrieval/influxdb_client.py` with Flux CSV parsing
   - **Integration**: Logs are automatically retrieved and added to context chunks when InfluxDB is configured
   - **Configuration**: Via environment variables (INFLUXDB_URL, INFLUXDB_TOKEN, INFLUXDB_ORG, INFLUXDB_BUCKET)
   - **Note**: InfluxDB integration is optional - system works without it. If not configured, log retrieval is skipped gracefully

### Data Ingestion Requirements

**Parser/Normalizer Architecture**:
- **Framework**: Python (FastAPI service) with Pydantic schema validation
- **Configuration-Driven Field Mapping**: Uses `config/field_mappings.json` for all field mappings  **IMPLEMENTED**
  - **ServiceNow CSV**: Maps CSV columns to internal models via configuration
  - **Runbook DOCX**: Maps DOCX sections to internal models via configuration
  - **Easy Extension**: Add new columns/sections by updating JSON config, no code changes needed
- **Ingestion Scripts**:  **CREATED**
  - **CSV Ingestion**: `scripts/data/ingest_servicenow_tickets.py` - Reads field mappings from config
  - **DOCX Ingestion**: `scripts/data/ingest_runbooks.py` - Reads field mappings from config
- **Runbook Parsing**: JSON schema-driven extraction (steps, commands, sections)
  - Extracts structured data: title, steps[], commands[], prerequisites[], rollback_procedures[]
  - Uses field mappings from `config/field_mappings.json`
  - Validates against JSON schema
- **Incident Parsing**: JSON schema-driven extraction
  - Extracts key fields from ServiceNow CSV using field mappings
  - Maps CSV columns (e.g., `assignment_group`, `cmdb_ci`, `short_description`) to internal fields
  - Normalizes ServiceNow ticket format to canonical incident format
  - Severity derivation from `impact` + `urgency` via mapping configuration
- **Tags Structure**:  **ENHANCED**
  - Normalizers populate comprehensive tags: type, service, component, env, runbook_id, incident_id, canonical_incident_key, risk, last_reviewed_at, ticket_id
  - All required fields from specification are populated when available
- **Logs**: Retrieved via InfluxDB (read-only cURL queries)
  - **NEW REQUIREMENT**: InfluxDB integration needed
  - Read-only queries to fetch relevant logs for context
  - Logs are secondary priority but integrated for comprehensive context
- **Alerts**: Normalized JSON (key fields + description)
  - Standardized alert format with required fields
- **Validation & QA**: Schema + rule-based checks
  - Min ≥120 tokens, max ≤360 tokens per chunk
  - Required tags must be present
  - Schema validation for structured data extraction

**Tags Structure** (Mandatory fields):
- `type`: Document type (runbook, incident, alert, log)
- `service`: Service name
- `component`: Component name
- `env`: Environment (production, staging, etc.)
- `runbook_id`: Runbook identifier (if applicable)
- `incident_id`: Incident identifier (if applicable)
- `canonical_incident_key`: Canonical key for incident matching
- `risk`: Risk level
- `last_reviewed_at`: Last review timestamp
- `ticket_id`: ServiceNow ticket ID (if applicable)

**Current State**: The ingestion system supports JSON/JSONL files, but needs to be extended to support:
- **CSV parsing** for ServiceNow tickets (convert CSV rows to incident documents with full tag structure)
- **DOCX parsing** for runbooks (extract text content from Word documents with JSON schema-driven extraction)
- **InfluxDB integration** for log retrieval (read-only cURL queries)

**Migration Path**:
1. Create CSV parser to convert ServiceNow tickets to `IngestIncident` format with comprehensive tags
2. Create DOCX parser to extract runbook content (steps, commands, sections) using JSON schema-driven extraction
3. Integrate InfluxDB for log retrieval (read-only queries)
4. Update ingestion scripts to handle these formats
5. Deprecate fake data generation scripts (keep for testing only)

### Deprecated: Fake Data Generation

**Note**: `scripts/data/generate_fake_data.py` is deprecated for production use. It should only be used for:
- Testing and development
- System validation when real data is unavailable
- Generating synthetic test scenarios

**Production systems should use real data from `tickets_data/` and `runbooks/` folders.**

---

## Data Flow

### Triage Flow

```
POST /api/v1/triage
  → api/v1/triage.py (endpoint)
    → agents/triager.py (agent)
      ├── retrieval/hybrid_search.py (context retrieval)
      ├── llm_client.py (LLM call)
      ├── guardrails.py (validation)
      ├── policy.py (policy evaluation)
      └── repositories/incident_repository.py (store)
        → database
```

**Key Steps**:
1. Retrieve context using hybrid search (vector + full-text)
2. Call LLM for triage analysis
3. Validate output against guardrails
4. Evaluate policy (or defer if `feedback_before_policy=true`)
5. Store incident with triage_output, triage_evidence, policy_band, policy_decision

### Resolution Flow

```
POST /api/v1/resolution?incident_id={id}
  → api/v1/resolution.py (endpoint)
    → agents/resolution_copilot.py (agent)
      ├── repositories/incident_repository.py (get incident)
      ├── Check policy: can_auto_apply and requires_approval
      ├── If approval needed: Raise ApprovalRequiredError (403)
      ├── retrieval/hybrid_search.py (context retrieval - runbook-heavy)
      ├── llm_client.py (LLM call)
      ├── guardrails.py (validation)
      └── repositories/incident_repository.py (update)
        → database
```

**Key Steps**:
1. Fetch incident from database (fresh fetch to get updated policy)
2. Check policy: Read `stored_policy_decision` from database
3. If `not can_auto_apply or requires_approval`: Raise `ApprovalRequiredError` (403)
4. Retrieve context (**prioritize runbooks** from `runbooks/` folder)
   - Runbooks are the primary source for resolution steps and recommendations
   - Historical incidents (ServiceNow tickets) provide additional context
5. Call LLM for resolution generation
6. Validate output against guardrails
7. Store resolution with resolution_output, resolution_evidence

**Important**: Resolution agent fetches fresh incident data to get updated `policy_band` and `triage_output` (in case user edited via feedback).

### Feedback Flow

```
PUT /api/v1/incidents/{id}/feedback
  → api/v1/feedback.py (endpoint)
    → services/feedback_service.py (service)
      ├── repositories/feedback_repository.py (store feedback)
      ├── If user_edited provided: Update incident.triage_output
      ├── If policy_band provided: Update policy
      │   ├── Recompute policy_decision with new band
      │   ├── Set can_auto_apply and requires_approval based on band
      │   └── repositories/incident_repository.py (update policy)
      └── Verify update by fetching incident again
```

**Key Steps**:
1. Store feedback in feedback table
2. If `user_edited` provided: Update `incident.triage_output` with user edits
3. If `policy_band` provided: 
   - Recompute policy_decision with new band
   - Update `incident.policy_band` and `incident.policy_decision`
   - Verify update succeeded
4. Return feedback confirmation

**Important**: 
- User can edit `triage_output` (same structure, no new fields)
- User can override `policy_band` (AUTO, PROPOSE, REVIEW)
- Updated `triage_output` is used by resolution agent on next call

### Approval Workflow

**Current Flow (Corrected)**:

1. **Triage** → Creates incident with policy_band (AUTO/PROPOSE/REVIEW) based on severity/confidence
2. **Policy Evaluation**:
   - **AUTO** (`can_auto_apply=True, requires_approval=False`): 
     - Resolution can be generated immediately
     - "Generate Resolution" button is enabled
     - NO feedback required
   - **PROPOSE/REVIEW** (`can_auto_apply=False, requires_approval=True`):
     - Resolution blocked until approval
     - "Generate Resolution" button is disabled
     - Feedback form shown for user review/edits
3. **User Approval** (only for PROPOSE/REVIEW):
   - User reviews/edits triage output via feedback form
   - User clicks "Approve & Generate Resolution"
   - Feedback endpoint with `policy_band="AUTO"` → Updates DB:
     - Sets `policy_band="AUTO"`
     - Sets `policy_decision.can_auto_apply=True`
     - Sets `policy_decision.requires_approval=False`
     - Updates `triage_output` if user made edits
4. **Resolution Generation**:
   - For AUTO: Directly enabled, user clicks "Generate Resolution"
   - For PROPOSE/REVIEW: Enabled after approval, auto-triggers or user clicks

**Key Rules**:
- **AUTO policy should NEVER require feedback** - if `can_auto_apply=True`, proceed directly
- **"Generate Resolution" button enabled when**:
  - Policy is AUTO (`can_auto_apply=True, requires_approval=False`), OR
  - Feedback has been provided (policy updated to AUTO via approval)
- **"Generate Resolution" button disabled when**:
  - Policy is PROPOSE/REVIEW and no feedback provided yet
  - Warning present (no matching evidence)
- **UI should NOT auto-trigger resolution** - user must click button explicitly

---

## Guardrails and Limitations

### Context Validation and Self-Contained System

**IMPLEMENTED**: The system enforces strict context validation to ensure it only uses ingested data (runbooks, incidents, logs) and never falls back to LLM training data.

#### Key Features:

1. **Strict Context Requirement**:
   - **Triage**: Requires minimum 1 context chunk before calling LLM
   - **Resolution**: Requires minimum 1 context chunk before calling LLM
   - If no chunks are found, the system **fails with clear error message** instead of proceeding
   - Error messages guide users to fix metadata mismatches or ingest more data

2. **Metadata Filtering Improvements**:
   - **Case-Insensitive Partial Matching**: `service: "database"` matches `"Database-SQL"`, `"Database"`, etc.
   - Uses PostgreSQL `LOWER()` and `LIKE` for flexible matching
   - Prevents exact-match failures that previously caused zero chunks to be retrieved
   - **Implementation**: `retrieval/hybrid_search.py` lines 56-69

3. **LLM Prompt Constraints**:
   - Prompts explicitly forbid using training data or external knowledge
   - If no context is provided, LLM must set `confidence=0.0` and indicate no evidence found
   - Commands must be copied directly from runbooks, not generated generically
   - **Implementation**: `ai_service/prompts.py` - Updated with CRITICAL CONSTRAINTS sections

4. **Provenance Enforcement**:
   - All resolution outputs must include `provenance[]` with actual `doc_id` and `chunk_id`
   - Provenance references are validated against retrieved context chunks
   - Invalid provenance references are filtered out and replaced with valid ones
   - **Implementation**: `ai_service/agents/resolution_copilot.py` lines 306-340

#### Error Handling:

When context validation fails:
- **Triage**: Raises `ValueError` with message:
  - If no documents in DB: "Cannot generate triage without context. No historical data found..."
  - If documents exist but no matches: "Cannot generate triage without context. Database has X documents, but none match..."
- **Resolution**: Raises `ValueError` with similar messages, emphasizing need for runbooks

#### Benefits:

- **Self-Contained**: System only uses your ingested data, never external LLM knowledge
- **Transparent**: Clear errors when data is missing or mismatched
- **Reliable**: No silent failures or generic outputs based on training data
- **Auditable**: All outputs reference actual chunks via provenance

### Purpose
Guardrails ensure:
- **Quality**: Consistent, actionable outputs
- **Consistency**: Standardized format across all incidents
- **Database constraints**: Reasonable field sizes
- **UI/Display**: Fits in user interfaces
- **Safety**: Prevents dangerous commands in resolutions

### Triage Output Limits

| Field | Limit | Purpose |
|-------|-------|---------|
| `summary` | Max 500 characters | Brief, scannable summary |
| `likely_cause` | Max 300 characters | Concise root cause explanation |
| `affected_services` | Max 10 items | Focused list of impacted services |
| `recommended_actions` | Max 10 items | Actionable, prioritized list |
| `severity` | Must be: `low`, `medium`, `high`, `critical` | Standardized severity levels |
| `category` | Must be: `database`, `network`, `application`, `infrastructure`, `security`, `other` | Standardized categories |
| `confidence` | Range: 0.0 to 1.0 | Confidence score validation |

**Required Fields**: `severity`, `category`, `confidence`, `summary`, `likely_cause`, `affected_services`, `recommended_actions`, `routing` (target), `reasoning` (target)

**Field Mapping to ServiceNow Ticket Data**:
- `category`: Maps directly to `category` field in CSV (e.g., "Monitoring/Alert", "Performance")
- `severity`: Derived from `impact` and `urgency` fields in CSV (1-5 scale, mapped to critical/high/medium/low)
- `summary`: Derived from `short_description` field in CSV
- `likely_cause`: Inferred from `description` and historical patterns (not directly in CSV)
- `affected_services`: Derived from `cmdb_ci` field in CSV (e.g., "Database-SQL", "Server")
- `recommended_actions`: Inferred from historical incidents and runbooks (not directly in CSV)
- `routing`: **Maps to `assignment_group` field in CSV** (e.g., "SE DBA SQL", "NOC", "SE Windows") - **REQUIRED**
- `reasoning`: Generated by LLM citing evidence chunks (not in CSV)
- `confidence`: Calculated by LLM based on evidence quality (not in CSV)

**Target Fields** (to be added):
- `routing`: Team queue assignment - **maps to `assignment_group` from ServiceNow tickets** (e.g., "SE DBA SQL", "NOC", "SE Windows")
- `reasoning`: Short natural text citing evidence chunks used in analysis

### Resolution Output Limits

| Field | Limit | Purpose |
|-------|-------|---------|
| `resolution_steps` | Min 1, Max 20 steps | Reasonable number of steps |
| `estimated_time_minutes` | Min 1, Max 1440 (24 hours) | Realistic time estimates |
| `commands` | Max 10 commands | Focused command list |
| `rollback_steps` | Max 10 steps | Reasonable rollback plan |
| `risk_level` | Must be: `low`, `medium`, `high` | Standardized risk levels |

**Safety Checks**:
- **Dangerous Commands Blocked**: `rm -rf`, `dd if=`, `mkfs`, `fdisk`, `format`, `drop database`, `truncate table`, `delete from`, `kill -9`
- **Destructive Patterns Detected**: Drop/delete/truncate/format/rm/remove, Kill/terminate/shutdown, Clear/purge/wipe
- **Rollback Required**: For `risk_level: "high"` or `"critical"`

**Required Fields**: `resolution_steps` (or `steps[]`), `estimated_time_minutes`, `risk_level`, `requires_approval`

**Field Mapping to Runbook and Historical Data**:
- `steps[]`: Ordered natural language actions (rename from `resolution_steps`)
  - **Source**: Extracted from runbook `steps[]` field (DOCX files) - primary source
  - **Fallback**: Inferred from historical incident `resolution_steps[]` (ServiceNow tickets)
- `commands_by_step`: Dict mapping step index to commands array (replace flat `commands` list)
  - **Source**: Extracted from runbook content - commands embedded in steps or separate commands section
  - **Format**: `{"0": ["cmd1", "cmd2"], "1": ["cmd3"]}` mapping step index to command array
  - **Note**: Commands are copied directly from runbooks when available
- `rollback_plan`: Optional rollback steps
  - **Source**: Extracted from runbook `rollback_procedures` field (DOCX files)
  - **Fallback**: Inferred from historical incidents with rollback patterns
- `estimated_time_minutes`: Time estimate for resolution
  - **Source**: Inferred from historical incident resolution times (ServiceNow tickets)
  - **Fallback**: Estimated based on number of steps and complexity
- `risk_level`: Risk assessment (low, medium, high)
  - **Source**: Derived from severity, commands (dangerous patterns), and historical patterns
  - **Factors**: Command types, rollback availability, historical incident outcomes
- `requires_approval`: Whether approval is needed
  - **Source**: Determined by Policy Gate based on `risk_level` and `severity`
  - **Not from data**: Policy-driven decision
- `confidence`: System's confidence in these steps (0.0-1.0) - **TARGET FIELD**
  - **Source**: Calculated by LLM based on:
    - Quality of runbook match (exact match vs. inferred)
    - Evidence chunk relevance scores (RRF scores)
    - Historical incident success patterns
- `reasoning`: Short explanation citing evidence chunks (rename from `rationale`) - **TARGET FIELD**
  - **Source**: Generated by LLM citing:
    - Specific runbook sections used
    - Historical incident patterns referenced
    - Log evidence (if available from InfluxDB)
- `provenance[]`: Array of `{doc_id, chunk_id}` references (structured audit trail) - **TARGET FIELD**
  - **Source**: References to retrieved chunks from Context Lake
    - `doc_id`: Document UUID from `documents` table (runbook, incident, log)
    - `chunk_id`: Chunk UUID from `chunks` table
  - **Purpose**: Audit trail showing which specific runbook sections, incident records, and log snippets were used

**Target Fields** (to be added/updated):
- `steps[]`: Ordered natural language actions (rename from `resolution_steps`)
- `commands_by_step`: Dict mapping step index to commands array (replace flat `commands` list)
- `confidence`: System's confidence in these steps (0.0-1.0)
- `reasoning`: Short explanation citing evidence chunks (rename from `rationale`)
- `provenance[]`: Array of `{doc_id, chunk_id}` references (structured audit trail)

### LLM Prompt Constraints

The LLM prompts (`ai_service/prompts.py`) include character limits to guide the model:
- `summary`: Maximum 500 characters
- `likely_cause`: Maximum 300 characters
- `affected_services`: Maximum 10 items
- `recommended_actions`: Maximum 10 items

**Note**: Limits are configurable in `config/guardrails.json`. If LLM consistently exceeds limits, either:
1. Update the prompt to be more explicit
2. Increase limits in `config/guardrails.json`

---

## Database Schema

### Tables

#### `documents`
- Source documents (runbooks, SOPs, historical incidents)
- Fields: `id`, `doc_type`, `title`, `content`, `metadata`, `created_at`

#### `chunks`
- Chunked documents with embeddings and tsvector
- Fields: `id`, `document_id`, `chunk_index`, `content`, `embedding`, `fulltext_vector`, `metadata`

#### `incidents`
- Alert triage and resolution data
- Fields:
  - `id`: UUID primary key
  - `alert_id`: Original alert ID
  - `raw_alert`: JSONB - Original alert data
  - `triage_output`: JSONB - Triage analysis result
  - `triage_evidence`: JSONB - Evidence chunks used by triager agent
  - `resolution_output`: JSONB - Resolution steps
  - `resolution_evidence`: JSONB - Evidence chunks used by resolution copilot agent
  - `policy_band`: TEXT - AUTO, PROPOSE, or REVIEW (or PENDING if deferred)
  - `policy_decision`: JSONB - Full policy decision JSON with `can_auto_apply`, `requires_approval`, etc.
  - `alert_received_at`: Timestamp
  - `triage_completed_at`: Timestamp
  - `resolution_proposed_at`: Timestamp
  - `resolution_accepted_at`: Timestamp

#### `feedback`
- Human-in-the-loop edits
- Fields:
  - `id`: UUID primary key
  - `incident_id`: Foreign key to incidents
  - `feedback_type`: TEXT - "triage" or "resolution"
  - `system_output`: JSONB - Original system output
  - `user_edited`: JSONB - User-edited version
  - `notes`: TEXT - User notes
  - `created_at`: Timestamp

#### `incident_metrics`
- View for MTTR calculations
- Computed fields: `triage_secs`, `resolution_proposed_secs`, `mttr_secs`

### Database Connection

- **Connection Pooling**: `db/connection.py` implements connection pooling using `psycopg_pool`
  - Pool initialized on service startup via `init_db_pool(min_size=2, max_size=10)`
  - Configurable via environment variables: `DB_POOL_MIN` (default: 2), `DB_POOL_MAX` (default: 10)
  - Connections are automatically returned to pool when using `get_db_connection_context()` context manager
  - Falls back to direct connections if pool not initialized (backward compatible)
- **Connection Retries**:
  - `get_db_connection()` retries transient `psycopg.OperationalError` failures using exponential backoff
  - Controlled via env vars: `DB_CONN_RETRIES` (default: 3), `DB_CONN_RETRY_BASE_DELAY` (default: 1s), `DB_CONN_RETRY_MAX_DELAY` (default: 5s)
  - Each attempt is logged with the attempt number and next backoff to simplify debugging
- **Connection Factory**: `db/connection.py::get_db_connection()` - Gets connection from pool
- **Context Manager**: `get_db_connection_context()` - Recommended for new code (auto-returns connection)
- **Row Factory**: `dict_row` (returns dictionaries, not tuples)
- **Important**: Always use `.get()` or dictionary access when reading query results

---

## Error Handling and Resilience

### Retry Logic

**LLM API Calls** (`ai_service/llm_client.py`):
- Automatic retry with exponential backoff for transient failures
- Retries on: `RateLimitError`, `APIConnectionError`, `APITimeoutError`, and 5xx server errors
- Configuration:
  - `MAX_RETRIES`: 3 attempts
  - `INITIAL_RETRY_DELAY`: 1.0 second
  - `MAX_RETRY_DELAY`: 60.0 seconds
  - `RETRY_EXPONENTIAL_BASE`: 2.0 (exponential backoff)
  - Jitter: Up to 10% random delay added to prevent thundering herd
- Metrics: Tracks retry attempts and distinguishes rate_limit errors from other errors

**Database Operations**:
- Connection pooling provides automatic connection recovery
- Connection acquisition now includes exponential backoff retry logic (see **Database Connection** section for configuration)
- `get_db_connection_context()` detects unhealthy cursors/connections and closes them instead of returning corrupted handles to the pool

### Custom Exceptions Hierarchy

```
NOCAgentError (base)
├── ConfigurationError
├── DatabaseError
├── RetrievalError
├── LLMError
├── PolicyError
├── ValidationError
│   ├── TriageValidationError
│   └── ResolutionValidationError
├── IncidentNotFoundError
└── ApprovalRequiredError
```

### Error Flow

1. **Repository** throws `DatabaseError` or `IncidentNotFoundError`
2. **Service** catches and re-raises (or handles business logic)
3. **API endpoint** catches and converts to `HTTPException`
4. Returns appropriate HTTP status code:
   - `400`: Bad Request (validation errors)
   - `403`: Forbidden (approval required)
   - `404`: Not Found (incident not found)
   - `422`: Unprocessable Entity (data preconditions not met - e.g., no historical data)
   - `500`: Internal Server Error

### User-Facing Error Guidance
- `ai_service/api/error_utils.py::format_user_friendly_error()` inspects exceptions and appends actionable hints.
- Triage and resolution endpoints use these hints when returning `500` errors so operators immediately know whether to:
  - Set `OPENAI_API_KEY`
  - Wait and retry after an upstream rate limit
  - Ingest historical data for better retrieval
  - Check PostgreSQL availability and credentials

### Error Response Format

```json
{
  "detail": "Error message"
}
```

For approval errors:
```json
{
  "detail": {
    "error": "approval_required",
    "message": "User approval required before generating resolution. Policy band: PROPOSE (from configuration), can_auto_apply: False, requires_approval: True. Please review the triage results for incident {incident_id} and approve before requesting resolution.",
    "incident_id": "uuid"
  }
}
```

---

## Key Implementation Details

### Early Stopping for Missing Data

**Triage Agent** (`ai_service/agents/triager.py`):
- If `len(context_chunks) == 0`:
  - Check if database is empty (`doc_count == 0`)
  - If empty: Raise `ValueError("No historical data found...")`
  - If not empty but no matches: Raise `ValueError("No matching evidence found...")`
- Prevents proceeding without evidence

**Resolution Agent** (`ai_service/agents/resolution_copilot.py`):
- Same logic when performing triage first (no incident_id)
- Ensures resolution doesn't proceed without evidence

### Policy Update Flow

**Feedback Endpoint** (`ai_service/api/v1/feedback.py`):
1. Store feedback in feedback table
2. If `user_edited` provided: Update `incident.triage_output` via `incident_service.update_triage_output()`
3. If `policy_band` provided:
   - Recompute `policy_decision` with new band (using user_edited triage_output if provided)
   - Set `can_auto_apply` and `requires_approval` based on band
   - Update database via `incident_service.update_policy()`
   - Verify update by fetching incident again

**Resolution Agent** (`ai_service/agents/resolution_copilot.py`):
1. Fetch fresh incident from database (line 189)
2. Update `existing_policy_band` from fresh fetch (line 191)
3. Update `triage_output` from fresh fetch (line 193) - may have been edited by user
4. Read `stored_policy_decision` from database
5. Use `can_auto_apply` and `requires_approval` from stored decision
6. If approval needed: Raise `ApprovalRequiredError`

### Database Update Verification

**Repository** (`ai_service/repositories/incident_repository.py`):
- `update_policy()` method:
  1. Gets current policy values for logging
  2. Updates database
  3. Commits transaction
  4. Verifies update by fetching again
  5. Logs before/after values for debugging
- `update_triage_output()` method:
  1. Checks if incident exists
  2. Updates `triage_output` column with user-edited version
  3. Commits transaction
  4. Logs update

**Important**: Uses dictionary access (`current.get("policy_band")`) because cursor uses `dict_row` factory. All `fetchone()` results are dictionaries, not tuples.

### Hybrid Search

**Implementation**: `retrieval/hybrid_search.py`
- **Vector Search**: OpenAI embeddings with pgvector cosine similarity
  - Uses pgvector `<=>` operator for cosine distance
  - Returns ranked results by similarity score
- **Full-Text Search**: PostgreSQL tsvector with ts_rank
  - Uses PostgreSQL full-text search with ranking
  - Returns ranked results by relevance score
- **RRF (Reciprocal Rank Fusion)**: Combines vector and full-text search results
  - Formula: `RRF_score = 1/(k + rank)` for each result set
  - Combines scores from both search methods
  - Finds the winner by fusing ranked lists
- **MMR (Maximal Marginal Relevance)**: Enforces diversity in result sets
  - Prevents redundant or very similar chunks
  - Ensures diverse coverage of topics
  - Applied after RRF to final result set
- **Metadata Filtering** ( **UPDATED**):
  - Uses case-insensitive partial matching: `LOWER(c.metadata->>'service') LIKE LOWER(%s)` with `%value%` pattern
  - Allows `service: "database"` to match `"Database-SQL"`, `"Database"`, etc.
  - Prevents exact-match failures that previously caused zero chunks to be retrieved
  - **Implementation**: `retrieval/hybrid_search.py` lines 56-69 - Filter building with flexible matching
- **Configuration**: Limits and weights from `config/retrieval.json`
  - `vector_weight`: Weight for vector similarity (default: 0.7)
  - `fulltext_weight`: Weight for full-text search (default: 0.3)
  - `limit`: Total number of chunks to return
  - `prefer_types`: Preferred document types for filtering/boosting
  - `max_per_type`: Maximum chunks per document type

### Chunking Strategy

**Implementation**:
- **Sentence-safe Split**: Tokenizer (tiktoken) targeting 180-320 tokens per chunk  **UPDATED**
- **Overlap**: 30 tokens between chunks (within 20-40 range)  **UPDATED**
- **Headers & Metadata**: Compact headers auto-appended (doc type, service/component, title, last_reviewed_at)  **UPDATED**
- **Validation**: Schema + rule-based checks (min ≥120 tokens, max ≤360 tokens, required tags present)
- Location: `ingestion/chunker.py::chunk_text()` and `ingestion/chunker.py::add_chunk_header()`

**Two-Level Chunking**:

1. **Client-Side Chunking** (Transport Layer):
   - Large documents (>900KB) are automatically chunked by lines before upload
   - Handles FastAPI's 1MB request body limit
   - Location: Data ingestion scripts (for large ServiceNow ticket descriptions or runbook content)

2. **Server-Side Chunking** (RAG Layer):
   - Token-based chunking using tiktoken
   - Optimized for embedding model limits (8191 tokens max)
   - Location: `ingestion/chunker.py::chunk_text()`

3. **Batch Embedding Generation**:
   - Processes up to 50 chunks per API call (instead of 1 at a time)
   - **10-100x faster** for large documents
   - Location: `ingestion/embeddings.py::embed_texts_batch()`

### Logging

- **Format**: `TIMESTAMP | LEVEL | MODULE:FUNCTION:LINE | MESSAGE`
- **Levels**: DEBUG, INFO, WARNING, ERROR, CRITICAL
- **Output**: Console (stdout) + Daily log files
- **Daily Log Files**: Automatically creates `logs/{service_name}_{YYYY-MM-DD}.log`
  - Rotates at midnight daily
  - Keeps 30 days of logs
  - Separate files for `ai_service` and `ingestion` services
- **Configuration**: `LOG_LEVEL`, `LOG_FILE` (optional), `LOG_DIR` (optional) environment variables

### Testing

**Test Scripts** (in `tests/` directory):
- `tests/test_triage_and_resolution.py` - End-to-end triage and resolution flow
- `tests/test_robusta_flow.py` - Simulates Robusta playbook flow without K8s
- `tests/simulate_alerts.py` - Simulates multiple alerts
- `tests/test_api.sh` - Quick API testing script
- `tests/test_triage_example.sh` - Example triage testing script

**Unit Tests**: `tests/` directory with pytest

**Data Ingestion Scripts**:
- `scripts/data/ingest_data.py` - Generic ingestion script (supports JSON/JSONL)
- **TODO**: Create CSV ingestion script for ServiceNow tickets
- **TODO**: Create DOCX ingestion script for runbooks

---

## Common Issues and Solutions

### Issue: Policy Update Not Working

**Symptoms**: Policy band remains old value after feedback update

**Causes**:
1. Database cursor returns dictionaries, not tuples - use `.get()` not `[0]`
2. Docker container running old code - rebuild with `docker-compose build --no-cache`
3. Exception silently caught - check logs for errors

**Solution**: 
- Use dictionary access: `current.get("policy_band")` not `current[0]`
- Rebuild Docker: `docker-compose build ai-service && docker-compose up -d ai-service`
- Check logs: `docker logs noc-ai-service --tail 100 | grep -E "(Policy|update)"`

### Issue: LLM Exceeds Character Limits

**Symptoms**: Validation error "Likely cause too long: 331 chars (max: 300)"

**Causes**:
1. LLM prompt doesn't mention limits
2. Limits too restrictive

**Solution**:
- Update prompt in `ai_service/prompts.py` to include limits
- Increase limits in `config/guardrails.json` if needed

### Issue: Resolution Still Requires Approval After Feedback

**Symptoms**: Resolution returns 403 even after feedback with `policy_band="AUTO"`

**Causes**:
1. Resolution agent not fetching fresh incident data
2. Policy update failing silently
3. Database update not committed

**Solution**:
- Ensure resolution agent fetches fresh incident (line 189 in `resolution_copilot.py`)
- Check logs for policy update errors
- Verify database update succeeded (check logs for verification messages)

---

## Development Guidelines

### Adding New Features

1. **API Endpoint**: Add to `ai_service/api/v1/`
2. **Business Logic**: Add to `ai_service/services/` or `ai_service/agents/`
3. **Data Access**: Add to `ai_service/repositories/`
4. **Configuration**: Add to appropriate `config/*.json` file
5. **Tests**: Add to `tests/` directory

### Ingesting Real Data

**ServiceNow Tickets (CSV)**:
1. Create CSV parser script: `scripts/data/ingest_servicenow_tickets.py`
2. Parse CSV rows and convert to `IngestIncident` format
3. Map ServiceNow fields to incident fields (title, description, category, severity, etc.)
4. Batch ingest via `/ingest/incident` endpoint

**Runbooks (DOCX)**:
1. Create DOCX parser script: `scripts/data/ingest_runbooks.py`
2. Extract text content from DOCX files (use `python-docx` library)
3. Parse structure (title, steps, prerequisites, rollback procedures)
4. Convert to `IngestRunbook` format
5. Ingest via `/ingest/runbook` endpoint

**Example Workflow**:
```bash
# Ingest ServiceNow tickets
python scripts/data/ingest_servicenow_tickets.py --dir tickets_data

# Ingest runbooks
python scripts/data/ingest_runbooks.py --dir runbooks
```

### Modifying Configuration

1. Edit appropriate `config/*.json` file
2. No code changes needed (configuration is loaded dynamically)
3. Restart service to reload (or use `reload_config()` if implemented)

### Database Changes

1. Create migration in `db/migrations/`
2. Run migration: `python scripts/db/run_migration.py`
3. Update repository methods if schema changes

### Adding New Guardrails

1. Add rules to `config/guardrails.json`
2. Update `ai_service/guardrails.py` to use new rules
3. Update prompts if needed to guide LLM

---

## Key Files Reference

### Core Files
- `ai_service/core/config_loader.py` - Configuration loading
- `ai_service/core/exceptions.py` - Custom exceptions
- `ai_service/core/logger.py` - Logging setup

### Agent Files
- `ai_service/agents/triager.py` - Triage agent implementation
- `ai_service/agents/resolution_copilot.py` - Resolution agent implementation

### Repository Files
- `ai_service/repositories/incident_repository.py` - Incident data access
- `ai_service/repositories/feedback_repository.py` - Feedback data access

### Service Files
- `ai_service/services/incident_service.py` - Incident business logic
  - `get_incident()` - Get incident by ID
  - `create_incident()` - Create new incident
  - `update_policy()` - Update policy_band and policy_decision
  - `update_triage_output()` - Update triage_output with user edits
  - `update_resolution()` - Update resolution_output
- `ai_service/services/feedback_service.py` - Feedback business logic

### Configuration Files
- `config/policy.json` - Policy gate configuration
- `config/guardrails.json` - Validation rules
- `config/llm.json` - LLM settings
- `config/retrieval.json` - Search settings
- `config/workflow.json` - Workflow behavior
- `config/schemas.json` - Data schemas

### Prompt Files
- `ai_service/prompts.py` - LLM prompt templates

---

---

## Deployment and Infrastructure

### Docker Compose Setup

The entire stack can be deployed using Docker Compose, including:
- **AI Service**: FastAPI backend
- **Ingestion Service**: Data ingestion API
- **PostgreSQL**: Database with pgvector extension

#### Services and Ports

| Service | Container Name | Port | Description |
|---------|---------------|------|-------------|
| AI Service | `noc-ai-service` | 8001 | FastAPI backend API |
| Ingestion Service | `noc-ai-ingestion` | 8002 | Data ingestion API |
| PostgreSQL | `noc-ai-postgres` | 5432 | Database |

#### Starting the Stack

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Rebuild after code changes
docker-compose up -d --build

# Stop all services
docker-compose down
```


### Backend Docker Setup

**Dockerfile** (root):
- Base: Python 3.9-slim
- Installs system dependencies (gcc, postgresql-client)
- Installs Python dependencies from `requirements.txt`
- Runs as non-root user (`appuser`)
- Exposes port 8001
- Health check endpoint

**Volumes**:
- `./config:/app/config:ro`: Configuration files (read-only)
- `./logs:/app/logs`: Log files

**Environment Variables**:
- `DATABASE_URL`: PostgreSQL connection string
- `POSTGRES_HOST`, `POSTGRES_PORT`, `POSTGRES_DB`, `POSTGRES_USER`, `POSTGRES_PASSWORD`: Database config
- `OPENAI_API_KEY`: OpenAI API key (required)
- `LOG_LEVEL`: Logging level (default: INFO)

### Network Configuration

All services are on the `noc-ai-network` bridge network:
- Services can communicate using container names (e.g., `ai-service:8001`)
- All services can access `postgres:5432`

### Data Persistence

**Volumes**:
- `postgres_data`: PostgreSQL data directory

**Note**: Logs are stored in `./logs` directory (mounted as volume).

### Development vs Production

**Development**:
- Backend can run locally: `uvicorn ai_service.main:app --reload`
- Hot reload enabled for faster iteration

**Production (Docker)**:
- Backend runs in container (no hot reload)
- All services managed by docker-compose
- Single command to start entire stack

### Troubleshooting Docker

**Common Issues**:

1. **Container not starting**:
   ```bash
   docker-compose logs <service-name>
   docker-compose ps  # Check container status
   ```

2. **Code changes not reflected**:
   ```bash
   docker-compose build --no-cache <service-name>
   docker-compose up -d <service-name>
   ```

3. **Port conflicts**:
   - Check if ports 8001, 8002, 5432 are already in use
   - Modify ports in `docker-compose.yml` if needed

4. **Database connection issues**:
   - Ensure `postgres` service is healthy: `docker-compose ps postgres`
   - Check network connectivity: `docker-compose exec ai-service ping postgres`


---

## State-Based HITL System

### Overview

The system includes a state-based Human-In-The-Loop (HITL) integration that supports real-time agent workflows with state emission and pause/resume capabilities. **Note: We use WebSocket directly (NOT CopilotKit)** - CopilotKit was only used as a reference pattern for the HITL concept.

### HITL Implementation Pattern

The HITL system follows this pattern:
1. **Agent runs** → emits state snapshots at each step
2. **Hits review checkpoint** → emits state with `pending_action`
3. **State sent to UI** via WebSocket → UI renders review component
4. **Human approves/edits** → submits response via REST API
5. **Response fed back** → agent resumes from checkpoint
6. **Agent continues** → completes remaining steps

This is similar to CopilotKit/LangGraph patterns but implemented with our own WebSocket infrastructure.

### State Models (`ai_service/state/models.py`)

- **`AgentState`**: Canonical state model with incident metadata, current step, agent type, progress tracking, policy state, pending actions, logs, and error state
- **`PendingAction`**: Model for HITL actions awaiting human response
- **`ActionResponse`**: Model for human responses to actions
- **`AgentStep`**: Enum for agent execution steps (INITIALIZED, RETRIEVING_CONTEXT, CONTEXT_RETRIEVED, CALLING_LLM, LLM_COMPLETED, VALIDATING, VALIDATION_COMPLETE, POLICY_EVALUATING, POLICY_EVALUATED, PAUSED_FOR_REVIEW, RESUMED_FROM_REVIEW, STORING, COMPLETED, ERROR)

### State Bus (`ai_service/state/bus.py`)

- **`StateBus`**: Central state management system
  - Emits state snapshots to subscribers
  - Manages pending HITL actions
  - Handles pause/resume logic
  - Persists state to database
  - Thread-safe with async locks
  - Reloads non-completed states on startup so workflows survive restarts
  - Guards against duplicate resume calls (idempotent action tracking)
  - Background monitor escalates expired pending actions to `approve_policy` checkpoints and records timeout metrics

### State Repository (`ai_service/repositories/agent_state_repository.py`)

- **`AgentStateRepository`**: Database persistence for agent state
  - Save/load agent state
  - Query pending actions
  - Supports state recovery after restarts

### Agent State Endpoints (`ai_service/api/v1/agents.py`)

- **`GET /api/v1/agents/{incident_id}/state`**: Get current agent state
- **`WebSocket /api/v1/agents/{incident_id}/state`**: Real-time state streaming
- **`POST /api/v1/agents/{incident_id}/actions/{action_name}/respond`**: Respond to HITL action
- **`GET /api/v1/agents/{incident_id}/actions/pending`**: Get pending action

### State-Based Triage Agent (`ai_service/agents/triager_state.py`)

- **`triage_agent_state()`**: Async state-based triage agent
  - Emits state at each step
  - Pauses for HITL when `requires_approval=True`
  - Returns state in response

### State-Based Resolution Agent (`ai_service/agents/resolution_copilot_state.py`)

- **`resolution_agent_state()`**: Async analogue of `resolution_copilot_agent`
  - Requires an existing incident (triage first) to ensure triage/policy context
  - Emits the same state machine events (retrieval → LLM → validation → storing)
  - Persists provisional resolution output/evidence, then:
    - Auto-completes when `can_auto_apply=True` and `requires_approval=False`
    - Or pauses with a `review_resolution` action so a human can edit/approve before execution
  - Returns the latest `AgentState` plus `pending_action` metadata so the UI can display the resolution review form inline

### State Bus Persistence & Timeout Monitor

- On startup the global `StateBus` reloads any non-completed states from `agent_state`, rehydrates live WebSocket data, and repopulates the pending-action gauge so analysts can refresh the UI without losing progress.
- Pending actions are monitored via an async background task (configured in `ai_service/main.py` startup). When `expires_at` is reached, the bus marks the state as `ERROR`, removes the pending action, emits an updated snapshot, and records timeout metrics (`hitl_actions_total{status="timeout"}` + `hitl_action_duration_seconds`).
- When a reviewer responds, `resume_from_action()` now records the action duration and decrements `hitl_actions_pending`, guaranteeing the gauge stays accurate across restarts.
- The repository gained `list_states()` so recovery can happen deterministically, and the bus exposes `start()/stop()` for FastAPI lifecycle hooks.

### Database Schema

- **`agent_state` table** (migration `003_add_agent_state.sql`): Stores agent state snapshots
  - Columns: id, incident_id, agent_type, current_step, state_data (JSONB), pending_action (JSONB)
  - Indexes for efficient querying

### State-Based Flow

1. Client calls `POST /api/v1/triage?use_state=true`
2. Agent initializes state and emits `INITIALIZED`
3. Agent retrieves context → emits `RETRIEVING_CONTEXT` → `CONTEXT_RETRIEVED`
4. Agent calls LLM → emits `CALLING_LLM` → `LLM_COMPLETED`
5. Agent validates → emits `VALIDATING` → `VALIDATION_COMPLETE`
6. Agent evaluates policy → emits `POLICY_EVALUATING` → `POLICY_EVALUATED`
7. If approval needed: Agent pauses and emits `PAUSED_FOR_REVIEW`, creates `PendingAction`
8. Human responds via `POST /api/v1/agents/{incident_id}/actions/{action_name}/respond`
9. Agent resumes → emits `RESUMED_FROM_REVIEW` → continues to `STORING` → `COMPLETED`

### Frontend Integration (WebSocket-Based)

**Client Integration**:
- Clients can connect to WebSocket endpoint: `/api/v1/agents/{incident_id}/state`
- Receive real-time state updates as agents progress
- Submit responses via REST API: `/api/v1/agents/{incident_id}/actions/{action_name}/respond`
- State-based workflow supports pause/resume for human review

### Correct HITL Flow

**Backend**:
1. Agent emits state at each step via `state_bus.emit_state()`
2. When approval needed: `state_bus.pause_for_action()` creates `PendingAction`
3. Agent pauses execution (returns response with `pending_action`)
4. Backend waits for human response via REST API
5. On response: `state_bus.resume_from_action()` updates state
6. Agent continues execution (if implemented as async/resumable). For triage/resolution today, the pause simply hands control to the human reviewer; edits are persisted via `IncidentService` before resuming the state stream.

**Client (API Consumer)**:
1. Connect to WebSocket endpoint: `/api/v1/agents/{incident_id}/state`
2. Receive state updates in real-time
3. When `state.pending_action` exists: Present review form to user
4. User reviews/edits and submits via REST API
5. POST to `/api/v1/agents/{incident_id}/actions/{action_name}/respond`
6. Continue receiving state updates as agent resumes

**Important**: The agent doesn't actually "pause" in the traditional sense - it returns a response with `pending_action` and the frontend must call the resume endpoint to continue. For true pause/resume, the agent would need to be implemented as a state machine that can be resumed (future enhancement).

---

## Data Source Migration (2025-01-XX)

### Summary of Changes

The system has migrated from using **fake/synthetic data** to **real production data sources**:

1. **ServiceNow Tickets**: Real historical incident data in CSV format (`tickets_data/` folder)
   - Provides authentic triage context and patterns
   - Contains actual incident descriptions, categories, and resolutions
   - Needs CSV parsing script for ingestion

2. **Runbooks**: Real operational runbooks in DOCX format (`runbooks/` folder)
   - Primary source for resolution recommendations
   - Contains step-by-step procedures and best practices
   - Needs DOCX parsing script for ingestion

3. **Logs**: Secondary priority (can be ignored for now)

### Action Items

- [ ] Create CSV ingestion script for ServiceNow tickets
- [ ] Create DOCX ingestion script for runbooks
- [ ] Update retrieval configuration to prioritize runbooks for resolution
- [ ] Update prompts to reference real data patterns
- [ ] Deprecate fake data generation for production (keep for testing)

---

## Architecture Alignment Summary

### Current vs Target Architecture

The current implementation is **mostly aligned** with the target specification, with the following key differences:

#### Already Implemented
- FastAPI backend
- PostgreSQL with pgvector and tsvector
- RRF (Reciprocal Rank Fusion) for hybrid search
- MMR (Maximal Marginal Relevance) for diversity
- Pydantic schema validation
- JSON mode for LLM outputs (response_format: json_object)
- Token-based chunking with tiktoken
- Batch embedding generation
- Feedback loop with system_output, user_edited, diff storage
- Policy gates (AUTO/PROPOSE/REVIEW)
- Evidence tracking with provenance

#### Implementation Status
All core features and deliverables are **IMPLEMENTED**:
1. **LangGraph Framework**: **IMPLEMENTED** - Full integration with feature flag for backward compatibility
2. **Routing Field**: **COMPLETED** - Added to Alert Triager output, maps to ServiceNow `assignment_group`
3. **Resolution Output Structure**: **COMPLETED** - All fields implemented (`steps`, `commands_by_step`, `confidence`, `reasoning`, `provenance[]`, `rollback_plan`)
4. **Triage Output**: **COMPLETED** - All fields including `routing` and `reasoning`
5. **Chunking Parameters**: **COMPLETED** - 180-320 tokens with 30 overlap
6. **Embedding Model**: **CONFIGURATION READY** - Config-driven, migration script available for text-embedding-3-large
7. **LLM Model**: **COMPLETED** - GPT-4o-mini in JSON mode
8. **Tags Structure**: **COMPLETED** - Comprehensive tags populated by normalizers
9. **InfluxDB Integration**: **INTEGRATED** - Log retrieval integrated into both agents
10. **JSON Schema Validation**: **OPTIONAL VALIDATION ADDED** - JSON schemas created, optional validation available
11. **Data Ingestion Scripts**: **COMPLETED** - CSV and DOCX ingestion scripts with configuration-driven mappings

**All three main deliverables are fully functional and ready for use.**

---

**Last Updated**: 2025-01-XX
**Maintained By**: Development Team

